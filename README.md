# ğŸ§­ NeuroDrive-Vision  
### âš™ï¸ Advanced AI-Powered Lane Detection & Road Perception Framework  

---

## ğŸš€ Overview  

**NeuroDrive-Vision** ğŸš—ğŸ’¡ is an **AI-powered lane detection and road perception system** engineered to replicate the *core vision intelligence* of autonomous vehicles.  
It brings together the **precision of classical computer vision** and the **robustness of modern modular architecture**, enabling both **real-time performance** and **research-grade extensibility**.

### ğŸ§  Core Philosophy  
The goal of NeuroDrive-Vision is not just to detect lanes â€” but to **perceive the road like a driver would**, using a structured vision pipeline that interprets motion, geometry, and perspective in a consistent, explainable way.

Each stage of the system works as an intelligent, pluggable module that you can **enhance, swap, or train** for different environments (urban roads, highways, or race tracks).  
This makes the framework perfect for:
- ğŸ”¬ **Research & prototyping** of autonomous driving perception stacks  
- ğŸï¸ **Real-time applications** for road boundary detection  
- ğŸ§© **Educational demonstrations** in AI, OpenCV, and perception systems  

---

### ğŸŒŸ Key Highlights  
| ğŸ’¡ Feature | ğŸ§¾ Description |
|-------------|----------------|
| âš™ï¸ **End-to-End Pipeline** | From raw frame to annotated lane overlay â€” each step modularized and independently testable. |
| ğŸ§© **Adaptive ROI Mapping** | Automatically adjusts to different video resolutions and camera perspectives. |
| ğŸ§  **Exponential Smoothing (EMA)** | Maintains lane stability by learning temporal motion between frames. |
| ğŸš€ **Real-Time Performance** | Optimized OpenCV operations achieve 30â€“60 FPS on modern CPUs. |
| ğŸŒ **Dual Interface Support** | Command-Line (CLI) for automation and Gradio Web App for live visual demos. |
| ğŸ“¦ **Dockerized Deployment** | Build once, deploy anywhere â€” consistent behavior across environments. |
| ğŸ§° **Research-Grade Modularity** | Every component is decoupled, documented, and open for fine-tuning or replacement. |

---

### ğŸ§  Developed With  
- ğŸ§© **Python 3.11+** for modern type-safe design  
- ğŸ§® **NumPy** for high-speed numerical computation  
- ğŸ‘ï¸ **OpenCV** for robust image transformation and edge processing  
- ğŸ’» **Gradio** for web-based visualization and user interaction  
- ğŸ§± **Docker** for deployment and scalability  

> ğŸ§  Developed with â¤ï¸ by **mwasifanwar**  
> ğŸ”— _A vision system that learns, adapts, and drives._

---

## ğŸ§© Architecture  

The architecture of **NeuroDrive-Vision** is designed for **clarity, extensibility, and real-time flow**. Each module performs a focused task and hands off its output to the next â€” resulting in a clean and efficient processing pipeline.

---

### ğŸ§  High-Level System Flow  

ğŸš¦ **1. Frontend Layer**
- ğŸ–¥ï¸ **CLI Interface**: Enables headless operation for batch video processing and automation.
- ğŸŒ **Gradio Web Demo**: Provides a user-friendly browser interface for real-time experimentation.
- ğŸ’¬ Both interfaces invoke the same backend pipeline ensuring identical detection logic.

âš™ï¸ **2. Core Processing Layer**
- ğŸ§© The *heart* of the system, where all computer vision operations happen.
- Comprised of modular stages:
  - ğŸŸ£ **Edge Detection (Canny Algorithm)** â€” Extracts fine boundaries and gradients from raw frames.  
  - ğŸŸ¢ **Region of Interest (ROI) Extraction** â€” Masks irrelevant regions, focusing on lane-relevant areas.  
  - ğŸŸ¡ **Probabilistic Hough Transform** â€” Converts edge data into line candidates using polar coordinates.  
  - ğŸ”µ **Line Grouping & Averaging** â€” Filters and merges left/right lane boundaries.  
  - ğŸŸ  **Exponential Smoothing (EMA)** â€” Stabilizes detection across time for a fluid driving experience.

ğŸ§° **3. Shared Utilities & Configuration**
- ğŸ§¾ **Config Manager** â€” Stores and handles parameters (thresholds, kernel sizes, ROI shapes, etc.).
- ğŸ’¾ **Input/Output Helpers** â€” Manage frame capture, video writing, and batch processing.
- âš™ï¸ **Tracker Module (EMA-based)** â€” Tracks lane continuity between consecutive frames.
- ğŸ§® **Math & Geometry Utilities** â€” Support transformations and vector calculations.

ğŸ“¦ **4. Output & Visualization**
- ğŸ¥ Annotated video frames with smooth blue lane overlays.
- ğŸ“Š Optional visualization of each processing stage (edges, ROI, smoothed lanes).
- ğŸŒ Displayed interactively via Gradio or saved locally.

---

### ğŸ§¬ Architecture Diagram  

<p align="center">
  <img src="assets/figures/architecture.png" width="900" alt="System Architecture Diagram">
</p>

---

### ğŸ” Data Flow Summary  

| ğŸ”¢ Step | ğŸ§  Process | ğŸ¯ Output |
|:--------|:------------|:-----------|
| 1ï¸âƒ£ | Raw frame input via OpenCV | RGB frame |
| 2ï¸âƒ£ | Canny Edge Detection | Binary edge map |
| 3ï¸âƒ£ | Region of Interest Extraction | Masked edge image |
| 4ï¸âƒ£ | Hough Transform | Line segments |
| 5ï¸âƒ£ | Line Averaging & Filtering | Merged lane candidates |
| 6ï¸âƒ£ | Exponential Smoothing | Stable, smoothed lanes |
| 7ï¸âƒ£ | Overlay Rendering | Annotated driving frame |

---

### âš™ï¸ Architectural Philosophy  

> ğŸ’¬ **"Each module in the NeuroDrive pipeline is designed to think like a neuron â€” specialized, fast, and connected."**

- ğŸ§  **Parallelizable** â€” Future GPU or TensorRT integration ready.  
- ğŸ§© **Plug-and-Play Modules** â€” Swap Canny with Sobel, or add a DNN-based lane segmentation head.  
- ğŸ”„ **Bidirectional Data Flow** â€” Supports feedback loops for model fine-tuning and adaptive thresholds.  
- ğŸ§° **Layer Independence** â€” Frontend, core, and utilities operate independently with defined interfaces.  

---

âœ¨ **In essence:**  
The **Architecture** is built for **clarity**, **speed**, and **innovation** â€” enabling researchers, developers, and enthusiasts to extend the system into a full-scale perception module for self-driving vehicles.


## âœ¨ Key Features  

NeuroDrive-Vision isnâ€™t just a basic lane detector â€” itâ€™s a **complete AI perception framework** âš™ï¸ designed for performance, adaptability, and real-world scalability.  
Every feature is built with a clear purpose: to bridge the gap between academic research and production-grade autonomous systems.  

---

### ğŸ§© **1. Modular Vision Pipeline**  
> â€œEach component is independent yet perfectly synchronized.â€  

- Every stage â€” from **edge detection** to **smoothing** â€” exists as a self-contained module.  
- Developers can easily modify, replace, or extend parts of the pipeline without breaking the system.  
- Ideal for experimentation, allowing quick swaps (e.g., Canny â†’ Sobel â†’ Deep CNN lanes).  

ğŸ”¹ *Benefits:*  
- Easy debugging and testing per stage.  
- Seamless integration for AI/ML add-ons.  
- Encourages clean and scalable architecture.

---

### ğŸ—ºï¸ **2. Adaptive ROI Masking (Dynamic Field of View)**  
> â€œA vision system that adjusts to the road like a human eye.â€  

- The **Region of Interest (ROI)** dynamically scales with input frame dimensions.  
- Automatically aligns to different camera positions and angles â€” from dashboard cams to drones.  
- Guarantees the algorithm focuses only on lane-relevant zones, filtering out sky, mountains, or irrelevant objects.  

ğŸ¯ *Benefits:*  
- Works across any camera setup or road width.  
- Reduces computation and false positives.  
- Improves efficiency on embedded systems (Jetson, Pi, etc.).

---

### ğŸ“‰ **3. EMA Smoothing (Temporal Stability)**  
> â€œBecause roads move fast, but your detection shouldnâ€™t flicker.â€  

- Implements an **Exponential Moving Average (EMA)** smoother for both left and right lane coordinates.  
- Reduces noise and frame-to-frame jitter caused by sensor variations or uneven edges.  
- Mimics temporal prediction used in real autonomous vehicles for **lane persistence**.  

ğŸ§  *Benefits:*  
- Delivers buttery-smooth line motion even on shaky footage.  
- Helps maintain consistent visual alignment.  
- Future-ready for integration with **Kalman Filters** or **LSTM-based tracking**.  

---

### ğŸ§  **4. Dual Interface System (CLI + Web UI)**  
> â€œPower for engineers, simplicity for users.â€  

- ğŸ’» **Command-Line Interface (CLI):** Automate and batch-process videos with flexible argument controls.  
- ğŸŒ **Gradio Web Demo:** Experiment interactively, visualize outputs, and compare results live.  
- Both use the same backend logic, ensuring **identical performance** and results.  

âœ¨ *Benefits:*  
- Developer-friendly automation via CLI.  
- Non-technical users can explore the model visually.  
- Excellent for demos, research papers, and workshops.

---

### âš¡ **5. Real-Time Capable**  
> â€œOptimized to see, think, and respond â€” instantly.â€  

- Built with pure **NumPy** and **OpenCV vectorized operations** for speed.  
- Capable of processing 30â€“60 FPS on standard hardware.  
- Efficient memory management ensures no dropped frames or lag.  

ğŸš€ *Benefits:*  
- Ideal for real-time applications like robotics or ADAS prototypes.  
- No external dependencies on GPUs or heavy models.  
- Runs on laptops, Raspberry Pi, or embedded ARM devices.  

---

## ğŸ§± Project Structure  

The **NeuroDrive-Vision** repository is organized to maintain **clarity, scalability, and modularity**.  
Each folder serves a well-defined purpose â€” from pipeline logic to front-end interaction, ensuring seamless development, testing, and deployment.

---

### ğŸ“‚ Directory Overview  

<details>
<summary><b>Click to view directory tree</b></summary>

neurodrive-vision/
â”œâ”€ ğŸ“ src/                         # ğŸ§  Core source code for processing & orchestration
â”‚  â”œâ”€ âš™ï¸ config.py                  # Configuration dataclasses and tunable parameters
â”‚  â”œâ”€ ğŸ”„ pipeline.py               # Main orchestrator handling frame-by-frame logic
â”‚  â”œâ”€ ğŸ§° utils.py                  # Shared helper functions (math, blending, etc.)
â”‚  â”œâ”€ ğŸ’¾ io.py                     # Input/output utilities for videos & images
â”‚  â”œâ”€ ğŸ¯ tracker.py                # EMA-based lane tracker and smoother
â”‚  â””â”€ ğŸ“ vision/                   # Vision processing submodules
â”‚     â”œâ”€ ğŸ‘ï¸ edges.py               # Canny edge detection module
â”‚     â”œâ”€ ğŸ—ºï¸ roi.py                 # Dynamic Region-of-Interest (ROI) extraction
â”‚     â”œâ”€ ğŸ“ hough.py               # Probabilistic Hough transform logic
â”‚     â”œâ”€ ğŸ“‰ lines.py               # Line grouping, filtering, and averaging
â”‚     â””â”€ ğŸ” smoothing.py           # Exponential lane smoothing algorithm
â”‚
â”œâ”€ ğŸ“ apps/                        # ğŸ’» Interfaces for different usage modes
â”‚  â”œâ”€ ğŸ§¾ cli.py                    # Command-line interface (batch mode)
â”‚  â””â”€ ğŸŒ gradio_app.py             # Web interface (Gradio demo dashboard)
â”‚
â”œâ”€ ğŸ“ assets/                      # ğŸ§© Visual and data assets
â”‚  â”œâ”€ ğŸ“ figures/                  # ğŸ“Š Documentation figures & output samples
â”‚  â”‚  â”œâ”€ ğŸ§¬ architecture.png        # Complete system architecture diagram
â”‚  â”‚  â”œâ”€ ğŸŸ£ roi_mask_demo.png       # ROI region visualization
â”‚  â”‚  â”œâ”€ âšª canny_edges_full.png    # Full-frame edge detection result
â”‚  â”‚  â”œâ”€ ğŸŸ¢ canny_edges_roi.png     # ROI-filtered Canny result
â”‚  â”‚  â””â”€ ğŸ”µ lane_overlay_result.png # Final lane overlay output
â”‚  â””â”€ ğŸï¸ sample_video.mp4          # Example driving video for testing
â”‚
â”œâ”€ ğŸ“ tests/                       # ğŸ§ª Unit testing modules
â”‚  â””â”€ ğŸ§© test_edges.py             # Unit test for edge detection validation
â”‚
â”œâ”€ ğŸ“œ requirements.txt             # ğŸ“¦ Python dependencies
â”œâ”€ ğŸ³ Dockerfile                   # ğŸ§± Docker image for containerized deployment
â””â”€ ğŸ§¾ README.md                    # ğŸ“˜ Project documentation

</details>

âš™ï¸ Installation

ğŸª„ Quick Setup â€” Get started in under a minute!

Clone the repository and install dependencies:

# 1ï¸âƒ£ Clone the repository
git clone https://github.com/mwasifanwar/neurodrive-vision.git

# 2ï¸âƒ£ Navigate into the project folder
cd neurodrive-vision

# 3ï¸âƒ£ Create a virtual environment
python -m venv .venv

# 4ï¸âƒ£ Activate the environment
# ğŸªŸ Windows
.venv\Scripts\activate
# ğŸ§ macOS / Linux
source .venv/bin/activate

# 5ï¸âƒ£ Install all required dependencies
pip install -r requirements.txt

## ğŸ–¼ï¸ Visual Results  

Every stage of **NeuroDrive-Visionâ€™s** processing pipeline produces a meaningful transformation in how the system perceives the road.  
These visuals are stored in the `assets/figures/` folder â€” each representing a critical step from raw frame to fully annotated output.  

They not only help debug and analyze the perception pipeline but also provide a transparent view of the modelâ€™s visual reasoning process.  

---

### ğŸ“Š **Processing Stages Overview**

| ğŸ§© **Stage** | ğŸ“– **Description** | ğŸ–¼ï¸ **Sample Output** |
|:--------------|:------------------|:----------------------|
| ğŸŸ£ **ROI Mask** | The Region of Interest isolates only the driving-relevant area (a triangular mask that focuses on the lane zone). Everything outside this region is darkened, ensuring efficient and accurate lane detection. | <p align="center"><img src="assets/figures/roi_mask_demo.png" width="800" alt="ROI Mask Visualization"></p> |
| âšª **Canny Edges** | The raw edge map generated using the **Canny Edge Detector** â€” highlights all strong gradients and lane boundaries. This serves as the foundation for subsequent lane extraction. | <p align="center"><img src="assets/figures/canny_edges_full.png" width="800" alt="Canny Edge Detection"></p> |
| ğŸŸ¢ **Masked Edges (Canny + ROI)** | The intersection of Canny edges with the ROI mask. This focuses computation only on edges inside the defined lane region, eliminating irrelevant patterns (trees, sky, etc.). | <p align="center"><img src="assets/figures/canny_edges_roi.png" width="800" alt="Canny Edges within ROI"></p> |
| ğŸ”µ **Final Overlay** | The final composited frame. The detected lane lines (left and right) are drawn in blue with smooth transitions using the **EMA-based smoothing algorithm**, resulting in stable and visually pleasing motion. | <p align="center"><img src="assets/figures/lane_overlay_result.png" width="800" alt="Final Annotated Lane Output"></p> |

---

### ğŸ” **How to Access These Outputs**

All processed visualization examples are saved under:

ğŸ“ assets/
 â”œâ”€ ğŸ“ figures/
 â”‚   â”œâ”€ roi_mask_demo.png          # Step 1 - ROI visualization
 â”‚   â”œâ”€ canny_edges_full.png       # Step 2 - Full edge map
 â”‚   â”œâ”€ canny_edges_roi.png        # Step 3 - ROI-filtered edge result
 â”‚   â””â”€ lane_overlay_result.png    # Step 4 - Final detected lane overlay

## âš™ï¸ Configuration  

NeuroDrive-Vision is built with a **fully modular and configurable core**, allowing you to fine-tune every stage of the perception pipeline with ease.  
All parameters are organized inside [`src/config.py`](src/config.py) ğŸ§© â€” designed using **Python dataclasses** for clarity and type safety.  

Each configuration block controls a specific part of the pipeline, ensuring that developers and researchers can quickly experiment with different thresholds, filters, and visual effects.  

---

### ğŸ§¾ **Configuration Categories**

| ğŸ”§ **Category** | âš™ï¸ **Parameters** | ğŸ§  **Description** |
|:----------------|:------------------|:------------------|
| ğŸ©¶ **Canny** | `blur_kernel`, `low_thresh`, `high_thresh` | Controls Gaussian blur size and edge detection thresholds for refining sensitivity to road boundaries. |
| ğŸ“ **Hough Transform** | `rho`, `theta`, `threshold`, `min_line_len`, `max_line_gap` | Adjusts line detection precision using polar coordinates; fine-tune for lane segment smoothness and continuity. |
| ğŸ—ºï¸ **Region of Interest (ROI)** | `bottom_left`, `top_mid`, `bottom_right` | Defines the triangular field of view where lanes are expected â€” adaptive to video resolution. |
| ğŸ“‰ **Smoothing (EMA)** | `ema_alpha` | Controls how smoothly lane positions evolve across frames â€” higher = faster response, lower = more stable. |
| ğŸ¨ **Drawing Settings** | `line_color`, `line_thickness`, `blend_alpha` | Customizes lane overlay aesthetics including color (BGR), thickness, and transparency blending ratio. |

---

## ğŸ”® Future Enhancements  

**NeuroDrive-Vision** is not a static project â€” itâ€™s an **evolving AI research platform** ğŸ§ , designed to keep growing alongside advancements in autonomous perception and computer vision.  
The roadmap focuses on transforming it from a classical CV-based system into a **hybrid AI perception framework** that seamlessly merges geometry, deep learning, and temporal intelligence.  

---

### ğŸŒŸ **Vision Statement**  
> â€œTo bridge the gap between classical computer vision and deep learning â€” creating a scalable, interpretable, and real-time perception system for next-generation autonomous driving.â€  

---

### ğŸ§  **Planned Upgrades & Research Roadmap**

| ğŸš€ **Feature** | ğŸ’¡ **Purpose & Description** |
|:----------------|:-----------------------------|
| ğŸŸ© **YOLOv8 / SegFormer Integration** | Integrate modern deep-learning-based segmentation models for **pixel-level lane understanding**. This enhancement will enable **robust lane detection** in complex environments (rain, glare, snow, or night driving). |
| ğŸ§® **Kalman Filter Tracking** | Introduce **state-space tracking and prediction** to enhance temporal lane stability. This addition will allow **predictive lane behavior** even when edges are temporarily obscured by vehicles or road artifacts. |
| ğŸŒ€ **Curvature Estimation Module** | Implement a **lane curvature and slope analysis** algorithm to calculate real-time bending angles of roads, ideal for adaptive steering simulations and robotics control systems. |
| âš¡ **FastAPI Microservice Deployment** | Package the detection pipeline as a **RESTful API** powered by **FastAPI**, enabling real-time streaming, mobile integration, and deployment in **autonomous simulation environments** (CARLA, Webots, etc.). |
| ğŸ¤– **ROS2 (Robot Operating System) Compatibility** | Build ROS2 nodes for plug-and-play integration with robotics systems, ensuring seamless synchronization with **camera feeds, IMU sensors, and LiDAR data** for holistic road understanding. |

---

### ğŸ§© **Beyond the Roadmap**

The long-term vision for **NeuroDrive-Vision** goes beyond simple lane tracking â€” aiming to evolve into a full-scale **AI Driving Perception Suite** featuring:

- ğŸ§­ **Multi-Lane Tracking** with probabilistic lane identity association  
- ğŸ§± **3D Lane Reconstruction** using stereo depth or LiDAR fusion  
- ğŸ§  **Reinforcement Learning-Based Control Feedback Loops**  
- â˜ï¸ **Cloud-Synced Dataset Labeling for Model Retraining**  
- ğŸ”„ **Self-Calibrating Parameter Optimization Engine**

---

### ğŸ’¬ **Why This Matters**

These planned enhancements will transform **NeuroDrive-Vision** from a research tool into a **real-world deployable perception backbone**, capable of powering:

- ğŸš— **Autonomous Driving Simulators** (CARLA, AirSim)  
- ğŸ¤– **Robotic Navigation Systems**  
- ğŸ§  **AI-Powered Traffic Analysis Tools**  
- ğŸ›°ï¸ **Smart-City Vision Applications**

---

<br>

<h2 align="center">âœ¨ Author</h2>

<p align="center">
  <b>Muhammad Wasif</b><br>
 AI/ML Developer @ Effixly AI
</p>

<p align="center">
  <a href="https://www.linkedin.com/in/mwasifanwar" target="_blank">
    <img src="https://img.shields.io/badge/LinkedIn-blue?style=for-the-badge&logo=linkedin" alt="LinkedIn">
  </a>
  <a href="mailto:wasifsdk@gmail.com">
    <img src="https://img.shields.io/badge/Email-grey?style=for-the-badge&logo=gmail" alt="Email">
  </a>
  <a href="https://mwasif.dev" target="_blank">
    <img src="https://img.shields.io/badge/Website-black?style=for-the-badge&logo=google-chrome" alt="Website">
  </a>
</p>

<p align="center">
  <em>â­ *"A machine that can see the road is not just detecting pixels â€”  
> itâ€™s learning to understand purpose, motion, and possibility."*</em>  
</p>

<br>

